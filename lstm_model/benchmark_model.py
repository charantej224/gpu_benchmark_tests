# -*- coding: utf-8 -*-
"""model_training-lstm-scratch_v1_1_ll.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10IpJURnKWepoSmscqybOo_ZkFI6v22Et

## Multi Input Binary Training (LSTM + DNN)
    - author: Lance Lukens
    - notebook used for L-Sprint LSTM testing
    - Tensorflow 2.2.0
    - Using Functional API
    - LSTM embedding approach on mgr txt nlp
    https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html
"""
# check for gpu devices
from tensorflow.python.client import device_lib

print(device_lib.list_local_devices())

import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.utils import class_weight

# from datetime import datetime
homepath = './'

from lstm_model.faultalarmclasses_cp4d import *
from lstm_model.partslib_cp4d import *

print(os.environ.get('HOSTNAME'))

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

current_directory = os.path.dirname(os.path.abspath(__file__))

"""## Load Data, Feature Selection
- removed drms, nodealias, manager
"""

baseline_version = 'v3.4.5.0_baseline'
x_train_path = os.path.join(current_directory, f'data/{baseline_version}/data/binary_X_train.pkl')
y_train_path = os.path.join(current_directory, f'data/{baseline_version}/data/binary_y_train.pkl')
x_test_path = os.path.join(current_directory, f'data/{baseline_version}/data/binary_X_test.pkl')
y_test_path = os.path.join(current_directory, f'data/{baseline_version}/data/binary_y_test.pkl')

X_train = pd.read_pickle(x_train_path)
y_train = pd.read_pickle(y_train_path)
X_test = pd.read_pickle(x_test_path)
y_test = pd.read_pickle(y_test_path)

x_labels = ['REGION', 'MARKET', 'SERVICE', 'ORIGINALSEVERITY',
            'SUPPRESSESCL', 'CLASSID', 'FMSKEYWORD', 'ALERTNAME', 'ALERTGROUP', 'THRESHOLD',
            'TECHNOLOGY', 'BACKHAULACCESSTYPE', 'FREQBAND', 'OEMMARKETVENDOR',
            'HIST_P170118', 'HIST_P054997', 'HIST_P058125', 'HIST_P013277',
            'HIST_P172008', 'HIST_P170345', 'HIST_P058566', 'HIST_P193814',
            'HIST_P009627', 'HIST_P058122', 'HIST_P179994', 'HIST_P206892',
            'HIST_P010068', 'HIST_P186481', 'HIST_P206893', 'HIST_P010203',
            'HIST_P021979', 'HIST_P009788', 'HIST_P192956', 'HIST_P054236',
            'HIST_P054991', 'HIST_P054295', 'HIST_P010602', 'HIST_P056865',
            'HIST_P058123', 'EQP_P170118', 'EQP_P054997', 'EQP_P058125',
            'EQP_P013277', 'EQP_P172008', 'EQP_P170345', 'EQP_P058566',
            'EQP_P193814', 'EQP_P009627', 'EQP_P058122', 'EQP_P179994',
            'EQP_P206892', 'EQP_P010068', 'EQP_P186481', 'EQP_P206893',
            'EQP_P010203', 'EQP_P021979', 'EQP_P009788', 'EQP_P192956',
            'EQP_P054236', 'EQP_P054991', 'EQP_P054295', 'EQP_P010602',
            'EQP_P056865', 'EQP_P058123']

"""## Sample Data IF required"""

# SET DATA SAMPLE SIZE HERE OPTIONAL IF WANT TO MERGE TRAIN/TEST DATA TO SAMPLE, ETC.
sampsize = 0.99


##############


def merge_traintest(X_tr, y_tr, X_ts, y_ts):
    # add labels
    X_tr['PARTS'] = y_tr['PARTS']
    X_ts['PARTS'] = y_ts['PARTS']

    # add new field, test/train split
    X_tr['TEST'] = 0
    X_ts['TEST'] = 1

    # vstack
    X_tr = pd.concat([X_tr, X_ts], axis=0)

    return X_tr


df = merge_traintest(X_train, y_train, X_test, y_test)
df = df.sample(frac=sampsize)

X_train = df.loc[df['TEST'] == 0, x_labels + ['IDENTIFIER', 'SUMMARY', 'MGR_SUGG_TRIAGE_TXT']]
y_train = df.loc[df['TEST'] == 0, ['PARTS']]

X_test = df.loc[df['TEST'] == 1, x_labels + ['IDENTIFIER', 'SUMMARY', 'MGR_SUGG_TRIAGE_TXT']]
y_test = df.loc[df['TEST'] == 1, ['PARTS']]

# Add four new fields to Train
X_train["EQP_P25_COUNT"] = X_train["EQP_P170118"].astype(int) + X_train["EQP_P054997"].astype(int) + X_train[
    "EQP_P058125"].astype(int) + X_train["EQP_P013277"].astype(int) + X_train["EQP_P172008"].astype(int) + X_train[
                               "EQP_P170345"].astype(int) + X_train["EQP_P058566"].astype(int) + X_train[
                               "EQP_P193814"].astype(int) + X_train["EQP_P009627"].astype(int) + X_train[
                               "EQP_P058122"].astype(int) + X_train["EQP_P179994"].astype(int) + X_train[
                               "EQP_P206892"].astype(int) + X_train["EQP_P010068"].astype(int) + X_train[
                               "EQP_P186481"].astype(int) + X_train["EQP_P206893"].astype(int) + X_train[
                               "EQP_P010203"].astype(int) + X_train["EQP_P021979"].astype(int) + X_train[
                               "EQP_P009788"].astype(int) + X_train["EQP_P192956"].astype(int) + X_train[
                               "EQP_P054236"].astype(int) + X_train["EQP_P054991"].astype(int) + X_train[
                               "EQP_P054295"].astype(int) + X_train["EQP_P010602"].astype(int) + X_train[
                               "EQP_P056865"].astype(int) + X_train["EQP_P058123"].astype(int)
X_train["EQP_P25"] = X_train["EQP_P25_COUNT"] > 0
X_train["EQP_P25"] = X_train["EQP_P25"].astype(int)

X_train["HIST_P25_COUNT"] = X_train["HIST_P170118"].astype(int) + X_train["HIST_P054997"].astype(int) + X_train[
    "HIST_P058125"].astype(int) + X_train["HIST_P013277"].astype(int) + X_train["HIST_P172008"].astype(int) + X_train[
                                "HIST_P170345"].astype(int) + X_train["HIST_P058566"].astype(int) + X_train[
                                "HIST_P193814"].astype(int) + X_train["HIST_P009627"].astype(int) + X_train[
                                "HIST_P058122"].astype(int) + X_train["HIST_P179994"].astype(int) + X_train[
                                "HIST_P206892"].astype(int) + X_train["HIST_P010068"].astype(int) + X_train[
                                "HIST_P186481"].astype(int) + X_train["HIST_P206893"].astype(int) + X_train[
                                "HIST_P010203"].astype(int) + X_train["HIST_P021979"].astype(int) + X_train[
                                "HIST_P009788"].astype(int) + X_train["HIST_P192956"].astype(int) + X_train[
                                "HIST_P054236"].astype(int) + X_train["HIST_P054991"].astype(int) + X_train[
                                "HIST_P054295"].astype(int) + X_train["HIST_P010602"].astype(int) + X_train[
                                "HIST_P056865"].astype(int) + X_train["HIST_P058123"].astype(int)
X_train["HIST_P25"] = X_train["HIST_P25_COUNT"] > 0
X_train["HIST_P25"] = X_train["HIST_P25"].astype(int)

# Add four new fields to Test
X_test["EQP_P25_COUNT"] = X_test["EQP_P170118"].astype(int) + X_test["EQP_P054997"].astype(int) + X_test[
    "EQP_P058125"].astype(int) + X_test["EQP_P013277"].astype(int) + X_test["EQP_P172008"].astype(int) + X_test[
                              "EQP_P170345"].astype(int) + X_test["EQP_P058566"].astype(int) + X_test[
                              "EQP_P193814"].astype(int) + X_test["EQP_P009627"].astype(int) + X_test[
                              "EQP_P058122"].astype(int) + X_test["EQP_P179994"].astype(int) + X_test[
                              "EQP_P206892"].astype(int) + X_test["EQP_P010068"].astype(int) + X_test[
                              "EQP_P186481"].astype(int) + X_test["EQP_P206893"].astype(int) + X_test[
                              "EQP_P010203"].astype(int) + X_test["EQP_P021979"].astype(int) + X_test[
                              "EQP_P009788"].astype(int) + X_test["EQP_P192956"].astype(int) + X_test[
                              "EQP_P054236"].astype(int) + X_test["EQP_P054991"].astype(int) + X_test[
                              "EQP_P054295"].astype(int) + X_test["EQP_P010602"].astype(int) + X_test[
                              "EQP_P056865"].astype(int) + X_test["EQP_P058123"].astype(int)
X_test["EQP_P25"] = X_test["EQP_P25_COUNT"] > 0
X_test["EQP_P25"] = X_test["EQP_P25"].astype(int)

X_test["HIST_P25_COUNT"] = X_test["HIST_P170118"].astype(int) + X_test["HIST_P054997"].astype(int) + X_test[
    "HIST_P058125"].astype(int) + X_test["HIST_P013277"].astype(int) + X_test["HIST_P172008"].astype(int) + X_test[
                               "HIST_P170345"].astype(int) + X_test["HIST_P058566"].astype(int) + X_test[
                               "HIST_P193814"].astype(int) + X_test["HIST_P009627"].astype(int) + X_test[
                               "HIST_P058122"].astype(int) + X_test["HIST_P179994"].astype(int) + X_test[
                               "HIST_P206892"].astype(int) + X_test["HIST_P010068"].astype(int) + X_test[
                               "HIST_P186481"].astype(int) + X_test["HIST_P206893"].astype(int) + X_test[
                               "HIST_P010203"].astype(int) + X_test["HIST_P021979"].astype(int) + X_test[
                               "HIST_P009788"].astype(int) + X_test["HIST_P192956"].astype(int) + X_test[
                               "HIST_P054236"].astype(int) + X_test["HIST_P054991"].astype(int) + X_test[
                               "HIST_P054295"].astype(int) + X_test["HIST_P010602"].astype(int) + X_test[
                               "HIST_P056865"].astype(int) + X_test["HIST_P058123"].astype(int)
X_test["HIST_P25"] = X_test["HIST_P25_COUNT"] > 0
X_test["HIST_P25"] = X_test["HIST_P25"].astype(int)

# new labels including the 4 new fields
x_labels1 = ['REGION', 'MARKET', 'SERVICE', 'ORIGINALSEVERITY',
             'SUPPRESSESCL', 'CLASSID', 'FMSKEYWORD', 'ALERTNAME', 'ALERTGROUP', 'THRESHOLD',
             'TECHNOLOGY', 'BACKHAULACCESSTYPE', 'FREQBAND', 'OEMMARKETVENDOR',
             'HIST_P170118', 'HIST_P054997', 'HIST_P058125', 'HIST_P013277',
             'HIST_P172008', 'HIST_P170345', 'HIST_P058566', 'HIST_P193814',
             'HIST_P009627', 'HIST_P058122', 'HIST_P179994', 'HIST_P206892',
             'HIST_P010068', 'HIST_P186481', 'HIST_P206893', 'HIST_P010203',
             'HIST_P021979', 'HIST_P009788', 'HIST_P192956', 'HIST_P054236',
             'HIST_P054991', 'HIST_P054295', 'HIST_P010602', 'HIST_P056865',
             'HIST_P058123', 'EQP_P170118', 'EQP_P054997', 'EQP_P058125',
             'EQP_P013277', 'EQP_P172008', 'EQP_P170345', 'EQP_P058566',
             'EQP_P193814', 'EQP_P009627', 'EQP_P058122', 'EQP_P179994',
             'EQP_P206892', 'EQP_P010068', 'EQP_P186481', 'EQP_P206893',
             'EQP_P010203', 'EQP_P021979', 'EQP_P009788', 'EQP_P192956',
             'EQP_P054236', 'EQP_P054991', 'EQP_P054295', 'EQP_P010602',
             'EQP_P056865', 'EQP_P058123', 'EQP_P25_COUNT', 'EQP_P25', 'HIST_P25_COUNT', 'HIST_P25']

# set word embedding paramaters

MAX_SEQUENCE_LENGTH = 100
MAX_NB_WORDS = 20000
EMBEDDING_DIM = 100


# download new index....slow download
# !wget http://nlp.stanford.edu/data/glove.6B.zip

def set_embeddings_index():
    embeddings_index = pd.DataFrame()
    f = open(os.path.join('data/', 'glove.6B.100d.txt'))
    counter = 0
    for line in f:
        counter += 1
        print('reading Embedding' + str(counter))
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()

    return embeddings_index


embeddings_index = set_embeddings_index()
print('Found %s word vectors.' % len(embeddings_index))


class TokenizerTransformer(BaseEstimator, TransformerMixin, Tokenizer):

    def __init__(self, num_words, maxlen):
        Tokenizer.__init__(self, num_words)
        self.maxlen = maxlen

    def fit(self, X, y=None):
        self.fit_on_texts(X)
        return self

    def transform(self, X, y=None):
        X_transformed = self.texts_to_sequences(X)
        X_padded = pad_sequences(X_transformed, maxlen=self.maxlen)
        return X_padded


def EmbedTransformer(X, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH):
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(X)
    sequences = tokenizer.texts_to_sequences(X)
    print('Shape of data tensor:', X.shape)

    word_index = tokenizer.word_index
    print('Found %s unique tokens.' % len(word_index))

    print('Preparing embedding matrix.')
    # prepare embedding matrix
    num_words = min(MAX_NB_WORDS, len(word_index) + 1)
    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
    for word, i in word_index.items():
        if i >= MAX_NB_WORDS:
            continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector

    return num_words, embedding_matrix


def ohe_col(df):
    # train data
    df_processed = pd.get_dummies(df, prefix_sep="__", columns=df.columns)

    cat_dummies = [col for col in df_processed
                   if "__" in col
                   and col.split("__")[0] in df.columns]

    processed_columns = list(df_processed.columns[:])

    with open(homepath + './ohe_dummies.txt', 'w') as filehandle:
        for listitem in cat_dummies:
            filehandle.write('%s\n' % listitem)

    with open(homepath + './ohe_columns.txt', 'w') as filehandle:
        for listitem in processed_columns:
            filehandle.write('%s\n' % listitem)

    return processed_columns


def ohe_xfrm(df):
    cat_columns = []
    ohe_dummies = []

    # open file and read the content in a list
    with open(homepath + './ohe_dummies.txt', 'r') as filehandle:
        for line in filehandle:
            # remove linebreak which is the last character of the string
            currentPlace = line[:-1]
            # add item to the list
            ohe_dummies.append(currentPlace)

    # open file and read the content in a list
    with open(homepath + './ohe_columns.txt', 'r') as filehandle:
        for line in filehandle:
            # remove linebreak which is the last character of the string
            currentPlace = line[:-1]
            # add item to the list
            cat_columns.append(currentPlace)

    # test/validation/new data
    df = pd.get_dummies(df, prefix_sep="__", columns=df.columns)

    # Remove additional columns
    for col in df.columns:
        if ("__" in col) and (col.split("__")[0] in cat_columns) and col not in ohe_dummies:
            # print("Removing additional feature {}".format(col))
            df.drop(col, axis=1, inplace=True)

    # set new values to 0's
    for col in ohe_dummies:
        if col not in df.columns:
            # print("Adding missing feature {}".format(col))
            df[col] = 0

    # set order
    df = df[ohe_columns]

    return df


class OHETransformer(BaseEstimator, TransformerMixin):

    def fit(self, x, y=None):
        return self

    def transform(self, x):
        if isinstance(x, pd.DataFrame):
            return ohe_xfrm(x)

        return x


# define custom transformers

# binary model column transformer settings
transformers_binary = [
    # ('ohe', OneHotEncoder(categories='auto', handle_unknown='ignore', sparse=False),x_labels),
    # ('ohe', OHETransformer(),x_labels),
    ('ohe', OHETransformer(), x_labels),
    ('SS', StandardScaler(), ['EQP_P25_COUNT', 'EQP_P25', 'HIST_P25_COUNT', 'HIST_P25']),
    ('tokenizer', TokenizerTransformer(num_words=MAX_NB_WORDS, maxlen=MAX_SEQUENCE_LENGTH), "MGR_SUGG_TRIAGE_TXT"),
]

df_clean = CleanTransformer("binary")
ct_xfr = ColumnTransformer(transformers_binary[:], remainder="drop", sparse_threshold=0, transformer_weights=None)

# data prep pipeline only!
pipeline = Pipeline(steps=[
    ('clean', df_clean),
    ('transform', ct_xfr)
])

# create ohe columns for column sorting
ohe_columns = ohe_col(X_train[x_labels])
# num_columns = len(numeric_df.columns)
num_columns = 100

# input shape set here!
INPUT_SHAPE = len(ohe_columns) + EMBEDDING_DIM + num_columns
print('INPUT_SHAPE is: ', INPUT_SHAPE)

# build word matrix for LSTM layer embedding
num_words, embedding_matrix = EmbedTransformer(X_train['MGR_SUGG_TRIAGE_TXT'], MAX_NB_WORDS, MAX_SEQUENCE_LENGTH)

from sklearn.model_selection import train_test_split

# split X_test, y_test:
# X_val,y_val for training validation.
# X_eval,y_eval for true test evaluation
X_val, X_eval, y_val, y_eval = train_test_split(X_test, y_test, test_size=0.50, random_state=42)

# transform all data used for train
X_tr = pipeline.fit_transform(X_train, y_train)

# transform all data used for validate
X_val = pipeline.fit_transform(X_val, y_val)

print("before resampling:\n", y_train['PARTS'].value_counts(normalize=True) * 100)

# oversample here
from imblearn.over_sampling import RandomOverSampler

rs = RandomOverSampler(random_state=12)
X_tr_res, y_train_res = rs.fit_sample(X_tr, y_train)

print("\nafter resampling:\n", y_train_res['PARTS'].value_counts(normalize=True) * 100)

# change to categorical
y_tr = to_categorical(y_train)
y_test = to_categorical(y_test)
y_val = to_categorical(y_val)

y_tr_res = to_categorical(y_train_res)

weights = class_weight.compute_class_weight('balanced', np.unique(y_tr[:, 1]), y_tr[:, 1])
# weights
class_weights = {0: weights[0], 1: weights[1]}
print(f"******** class_weight type is {type(class_weights)}, its value is {class_weights}\n")

"""## Model Training
- set model version name, settings run.
- trains model with callbacks
- don't forget to save model
"""

# set model name for test/training
model_version = 'vx.x.0.x_ll'

# setup callbacks for logging or checkpoints
callback_filepath = homepath + '/log/' + model_version
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=callback_filepath, write_graph=False)
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=callback_filepath, save_weights_only=True,
                                                               monitor='val_precision', mode='max', save_best_only=True)

# multi-gpu strategy
# https://www.tensorflow.org/guide/gpu
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    inputs = Input(shape=(INPUT_SHAPE,))

    nlp_input = inputs[:, inputs.shape[1] - MAX_SEQUENCE_LENGTH:]
    meta_input = inputs[:, :inputs.shape[1] - MAX_SEQUENCE_LENGTH]

    print('nlp_input:', nlp_input.shape)  # shape of nlp input
    print('meta_input:', meta_input.shape)  # shape of meta input

    emb = Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,
                    trainable=False)(nlp_input)
    nlp_out = LSTM(128, dropout=0.3, recurrent_dropout=0.3)(emb)
    x = concatenate([nlp_out, meta_input])
    x = Dense(83, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(2, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=[x])
    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy', 'Precision', 'Recall'])

# train model
print(model_version)
model.summary()

# fit model
# change epochs for longer runs
history = model.fit(X_tr, y_tr, epochs=100, batch_size=128, callbacks=[tensorboard_callback],
                    validation_data=(X_val, y_val), verbose=1)

"""### PLOT LEARNING CURVE"""

# plot learning curve lstm_1 - 25 epochs
# v3.5.5.0_baseline_yp_lstm_1 - with added four fields: EQP_P25_COUNT, EQP_P25, HIST_P25_COUNT, HIST_P25
loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs = history.epoch
plt.plot(epochs, loss_train, 'g', label='Training Accuracy')
plt.plot(epochs, loss_val, 'b', label='validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig("Epoch_Accuracy.png")
plt.show()
